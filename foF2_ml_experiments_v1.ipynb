{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os \n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Flatten, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stations' data preparation\n",
    " \n",
    "hour_intervals=[1,2,12,24,36,48]   # end of this cell 0 is added for original input dataset\n",
    "hour_steps_for_height=39 #39 is the number of different heights\n",
    "\n",
    "data_folder=\"data\\\\\"\n",
    "stations=[\"WUHN2009_foF2.txt\",\"BJNM2009_foF2.txt\",\"BJNM2015_foF2.txt\"]\n",
    "\n",
    "\n",
    "stations_data={}\n",
    "\n",
    "for st in stations:\n",
    "    stations_data[st]=pd.read_csv(data_folder+st, sep =',').iloc[::hour_steps_for_height]\n",
    "    print(\"\\n\\n--------------------------\"+st+\"--------------------------------------\\n\")\n",
    "    print(stations_data[st].shape)\n",
    "    print(stations_data[st].head(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input-Output data preparation,split and scaling for ML Models\n",
    "\n",
    "X_train_base,X_val_base,X_test_base, y_train_base,y_val_base, y_test_base={},{},{},{},{},{}\n",
    "\n",
    "num_fold=4\n",
    "\n",
    "train_ratio=0.8\n",
    "val_ratio=0.1\n",
    "test_ratio=0.1\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for st in stations_data:\n",
    "    \n",
    "    X, y = {}, {}\n",
    "    X[st] = stations_data[st].drop(['Date','Hour','DOY','Height','F10.7','EqDistance','foF2'], axis=1).to_numpy()\n",
    "    y[st] = stations_data[st]['foF2'].to_numpy()\n",
    "\n",
    "    # k-fold train_test split \n",
    "    X_train_base[st],X_val_base[st],X_test_base[st], y_train_base[st],y_val_base[st], y_test_base[st] \\\n",
    "        = np.empty((0,X[st].shape[1])),np.empty((0,X[st].shape[1])),np.empty((0,X[st].shape[1])),np.empty((0,)),np.empty((0,)),np.empty((0,))\n",
    "\n",
    "    for i in range(num_fold):\n",
    "\n",
    "        len_fold=int(X[st].shape[0]/num_fold)\n",
    "        split_index_train = len_fold*i+int(train_ratio * len_fold)\n",
    "        split_index_val = split_index_train+int(val_ratio * len_fold)\n",
    "\n",
    "\n",
    "        X_train_base[st]=np.vstack([X_train_base[st],X[st][i*len_fold:split_index_train]])\n",
    "        y_train_base[st]=np.hstack([y_train_base[st],y[st][i*len_fold:split_index_train]])\n",
    "        \n",
    "        X_val_base[st]=np.vstack([X_val_base[st],X[st][split_index_train:split_index_val]])\n",
    "        y_val_base[st]=np.hstack([y_val_base[st],y[st][split_index_train:split_index_val]])\n",
    "        \n",
    "        X_test_base[st]=np.vstack([X_test_base[st],X[st][split_index_val:(i+1)*len_fold]])\n",
    "        y_test_base[st]=np.hstack([y_test_base[st],y[st][split_index_val:(i+1)*len_fold]])\n",
    "\n",
    "\n",
    "    #shapes\n",
    "    print(st)\n",
    "    print(\"Train: \",X_train_base[st].shape,y_train_base[st].shape,\"Validation:\",X_val_base[st].shape,y_val_base[st].shape,\"Test: \",X_test_base[st].shape,y_test_base[st].shape,\"\\n\")\n",
    "    \n",
    "    #scaling\n",
    "    scaler.fit(X_train_base[st])\n",
    "    X_train_base[st] = scaler.transform(X_train_base[st])\n",
    "    X_val_base[st] = scaler.transform(X_val_base[st])\n",
    "    X_test_base[st] = scaler.transform(X_test_base[st])\n",
    "\n",
    "    print(\"Scaled X_train: \",X_train_base[st][0],\"\\n\") #prints first row of X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or save results\n",
    "result_array = [['Train', 'Train', 'Train', 'Train','Validation', 'Validation', 'Validation', 'Validation', 'Test', 'Test', 'Test', 'Test'],\n",
    "          ['RMSE', 'R2', 'MAE', 'MAPE','RMSE', 'R2', 'MAE', 'MAPE','RMSE', 'R2', 'MAE', 'MAPE']]\n",
    "results_df=pd.DataFrame(index=pd.MultiIndex.from_arrays(result_array,names=[\"ExperimentData\",\"Metric\"]))\n",
    "save_row_index=0\n",
    "save_column_index=0\n",
    "\n",
    "\n",
    "def save_error_to_df(model_name, data_type, mse_result,r2_result,mae_result,mape_result):\n",
    "    \n",
    "    results_df.loc[(data_type, 'RMSE'), model_name] = mse_result\n",
    "    results_df.loc[(data_type, 'R2'), model_name] = r2_result\n",
    "    results_df.loc[(data_type, 'MAE'), model_name] = mae_result\n",
    "    results_df.loc[(data_type, 'MAPE'), model_name] = mape_result\n",
    "\n",
    "def printError(model_name, data_type, y_in, pred_in):\n",
    "    # Calculate ensemble metrics\n",
    "    print(\"\\n\",model_name + \" \" + data_type +' Model Performance:'+\"\\n\")\n",
    "    _rmse = np.sqrt(mean_squared_error(y_in, pred_in))\n",
    "    _r2 = r2_score(y_in, pred_in)\n",
    "    _mae = mean_absolute_error(y_in, pred_in)\n",
    "    _mape = np.mean(np.abs((y_in - pred_in) / y_in)) * 100\n",
    "    \n",
    "    print('RMSE:', _rmse)\n",
    "    print('R^2:', _r2)\n",
    "    print('MAE:', _mae)\n",
    "    print('MAPE:', _mape)\n",
    "\n",
    "    save_error_to_df(model_name,data_type,round(_rmse,2),round(_r2,2),round(_mae,2),round(_mape,2))\n",
    "\n",
    "def save_error_to_excel(st_result_header,first_run):\n",
    "    global save_row_index\n",
    "    global save_column_index\n",
    "    st_header_df=pd.DataFrame(data=st_result_header)\n",
    "\n",
    "    if first_run:\n",
    "        #Export all results to an excel file\n",
    "        st_header_df.to_excel(data_folder+\"foF2_ML_Experiments.xlsx\",startrow=save_row_index,startcol=save_column_index,index=False,header=False)\n",
    "        save_row_index+=1\n",
    "      \n",
    "        with pd.ExcelWriter(data_folder+\"foF2_ML_Experiments.xlsx\", mode=\"a\",if_sheet_exists=\"overlay\") as writer:\n",
    "            results_df.to_excel(writer,startrow=save_row_index,startcol=save_column_index)\n",
    "        save_row_index+=16\n",
    "    else :\n",
    "        with pd.ExcelWriter(data_folder+\"foF2_ML_Experiments.xlsx\", mode=\"a\",if_sheet_exists=\"overlay\") as writer:\n",
    "            st_header_df.to_excel(writer,startrow=save_row_index,startcol=save_column_index,index=False,header=False)\n",
    "            save_row_index+=1\n",
    "            results_df.to_excel(writer,startrow=save_row_index,startcol=save_column_index)\n",
    "            save_row_index+=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run=True\n",
    "\n",
    "for hour_interval in hour_intervals:\n",
    "    \n",
    "    for st in stations_data:\n",
    "\n",
    "        st_result_header=[st[:st.rfind('_')]+\"_\"+str(hour_interval)+\"h\"] #Results' header\n",
    "\n",
    "        # data preparation for different hours\n",
    "\n",
    "        X_train = X_train_base.copy()[st][:-hour_interval]\n",
    "        X_test = X_test_base.copy()[st][:-hour_interval]\n",
    "        X_val = X_val_base.copy()[st][:-hour_interval]\n",
    "\n",
    "        y_train = y_train_base.copy()[st][hour_interval:]\n",
    "        y_test = y_test_base.copy()[st][hour_interval:]\n",
    "        y_val = y_val_base.copy()[st][hour_interval:]\n",
    "        \n",
    "        print(\"\\nSTATION: \",st[:-9],\" HOUR: \",hour_interval)\n",
    "\n",
    "        #Linear Regression Model\n",
    "\n",
    "        reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "        reg_predTrain = reg.predict(X_train) \n",
    "        reg_errorTrain = y_train - reg_predTrain\n",
    "        printError(\"Linear Regression\",\"Train\", y_train, reg_predTrain)\n",
    "\n",
    "        reg_predVal = reg.predict(X_val) \n",
    "        reg_errorVal = y_val - reg_predVal\n",
    "        printError(\"Linear Regression\",\"Validation\", y_val, reg_predVal)\n",
    "\n",
    "        reg_predTest = reg.predict(X_test)\n",
    "        reg_errorTest = y_test - reg_predTest\n",
    "        printError(\"Linear Regression\",\"Test\", y_test, reg_predTest)\n",
    "\n",
    "\n",
    "         #Decision Tree Model \n",
    "        dectree = DecisionTreeRegressor(max_depth=12,min_samples_leaf=5,random_state=44).fit(X_train, y_train)\n",
    "\n",
    "        dectree_predTrain = dectree.predict(X_train) \n",
    "        dectree_errorTrain = y_train-dectree_predTrain\n",
    "        printError(\"Decision Tree\",\"Train\", y_train ,dectree_predTrain)\n",
    "\n",
    "        dectree_predVal = dectree.predict(X_val) \n",
    "        dectree_errorVal = y_val - dectree_predVal\n",
    "        printError(\"Decision Tree\",\"Validation\", y_val, dectree_predVal)\n",
    "\n",
    "        dectree_predTest = dectree.predict(X_test)\n",
    "        dectree_errorTest = y_test - dectree_predTest\n",
    "        printError(\"Decision Tree\",\"Test\", y_test, dectree_predTest)\n",
    "\n",
    "\n",
    "        # MLP model\n",
    "\n",
    "        #default parameters= loss:mse, learning_rate:adaptive, batch:200\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(12,4), activation='relu', solver='adam', max_iter=500,shuffle=False,random_state=44)\n",
    "        mlp.fit(X_train, y_train)\n",
    "\n",
    "        mlp_predTrain = mlp.predict(X_train)\n",
    "        mlp_errorTrain = y_train - mlp_predTrain\n",
    "        printError(\"MLP\",\"Train\", y_train, mlp_predTrain)\n",
    "\n",
    "        mlp_predVal = mlp.predict(X_val)\n",
    "        mlp_errorVal = y_val - mlp_predVal\n",
    "        printError(\"MLP\",\"Validation\", y_val, mlp_predVal)\n",
    "\n",
    "        mlp_predTest = mlp.predict(X_test)\n",
    "        mlp_errorTest = y_test- mlp_predTest\n",
    "        printError(\"MLP\",\"Test\",y_test, mlp_predTest)\n",
    "\n",
    "\n",
    "        #LSTM Model\n",
    "\n",
    "        X_lstm_train = np.expand_dims(X_train, axis = 2)\n",
    "        X_lstm_val = np.expand_dims(X_val, axis = 2)\n",
    "        X_lstm_test = np.expand_dims(X_test, axis = 2)\n",
    "        \n",
    "        y_lstm_train = np.expand_dims(y_train, axis = 1)\n",
    "        y_lstm_val = np.expand_dims(y_val, axis = 1)\n",
    "        y_lstm_test = np.expand_dims(y_test, axis = 1)\n",
    "\n",
    "        model_lstm = Sequential()\n",
    "        model_lstm.add(LSTM(36, activation='relu', input_shape=(X_lstm_train.shape[1], X_lstm_train.shape[2])))\n",
    "        model_lstm.add(Dense(12))\n",
    "        model_lstm.add(Dense(1))\n",
    "        model_lstm.compile(optimizer=Adam(0.001), loss='mae')\n",
    "\n",
    "        lstm_history = model_lstm.fit(X_lstm_train, y_lstm_train, epochs=200,batch_size=15, verbose=2, use_multiprocessing=True)\n",
    "        \n",
    "        lstm_predTrain = model_lstm.predict(X_lstm_train) \n",
    "        lstm_errorTrain = y_lstm_train - lstm_predTrain\n",
    "        printError(\"LSTM\",\"Train\", y_lstm_train, lstm_predTrain)\n",
    "\n",
    "        lstm_predVal = model_lstm.predict(X_lstm_val) \n",
    "        lstm_errorVal = y_lstm_val - lstm_predVal\n",
    "        printError(\"LSTM\",\"Validation\", y_lstm_val, lstm_predVal)\n",
    "\n",
    "        lstm_predTest = model_lstm.predict(X_lstm_test)\n",
    "        lstm_errorTest = y_lstm_test - lstm_predTest\n",
    "        printError(\"LSTM\",\"Test\", y_lstm_test, lstm_predTest)\n",
    "\n",
    "\n",
    "        save_error_to_excel(st_result_header,first_run)\n",
    "        first_run=False\n",
    "    \n",
    "\n",
    "    save_column_index+=10\n",
    "    save_row_index=0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50ca854c4c3efc08975ea3cb237e66ea0431a08d0f81600bb6a2a9be70ab2e48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
