{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "stations=[\"ISTA_2012.txt\"]  #1 station to parse, multiple stations to parse and merge\n",
    "DIST_filename=\"Dst_2000_2023.txt\" # it is already cleaned before processing\n",
    "datafolder=\"data\\\\\"\n",
    "\n",
    "hour_steps_for_height=39 #39 is the number of different heights\n",
    "X=0.8   \n",
    "past_steps=11\n",
    "ap_start_index=past_steps*hour_steps_for_height # starting point to use previous values\n",
    "delta_step=18\n",
    "delta_X_column_size = delta_step+1\n",
    "\n",
    "baseindex=delta_step*hour_steps_for_height+hour_steps_for_height  #delta_kps already starts from +1h(39 hoursteps), so we added hour_steps_for height\n",
    "baseindex2=baseindex-ap_start_index # for apX and dist to adjust indexes\n",
    "baseindex3=baseindex-hour_steps_for_height #for r_delta_h to adjust indexes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads IRI and Dist files.\n",
    "\n",
    "df_st_merged_foF2=pd.DataFrame()\n",
    "\n",
    "def read_edit_dist():\n",
    "    \n",
    "    df_dst=pd.read_csv(datafolder+DIST_filename,skiprows=17)\n",
    "\n",
    "    df_dst['DATE']=pd.to_datetime(df_dst['DATE'])\n",
    "\n",
    "    mask_for_24h_format = (df_dst['TIME'] == '00:00:00.000')\n",
    "    df_dst.loc[mask_for_24h_format,'TIME'] = df_dst.loc[mask_for_24h_format,'TIME'].replace('00:00:00.000','24:00:00.000',regex=True)\n",
    "    df_dst.loc[mask_for_24h_format,'DATE'] = df_dst.loc[mask_for_24h_format,'DATE']-timedelta(days=1)\n",
    "    df_dst.loc[mask_for_24h_format,'DOY'] = df_dst.loc[mask_for_24h_format,'DOY']-1\n",
    "\n",
    "    return df_dst\n",
    "\n",
    "df_dst=read_edit_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots rowxcol grid for foF2 dataset\n",
    "def plot_data(location,year,foF2_data,colSize,rowSize):\n",
    "#dont forget to iterate with step hour_steps_for_height=39 to see date over time.\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=rowSize, ncols=colSize, figsize=(24, 16))\n",
    "    fig.suptitle(location+' foF2 Dataset Plots For '+year, fontsize=16)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    row_index=0\n",
    "    col_index=0\n",
    "\n",
    "    for col in foF2_data.columns[4:-2]: #excluded foF2 and Distance \n",
    "        axs[row_index, col_index].plot(foF2_data[\"Date\"][::hour_steps_for_height],foF2_data[col][::hour_steps_for_height])\n",
    "        axs[row_index, col_index].set_title(col)\n",
    "\n",
    "        col_index+=1\n",
    "\n",
    "        if col_index==3:\n",
    "            col_index=0\n",
    "            row_index+=1\n",
    "\n",
    "    fig.savefig(\"graphs/\"+location+\"_\"+year+\"_foF2_dataset.jpg\",dpi=500,bbox_inches='tight')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kp Regression\n",
    "1. We need to achieve stationarity of the Kp index and take the first differences of the Kp index. This involves subtracting each value from its previous value, which can remove trends and make the data more stationary. The formula for the first differences in the Kp index is: ΔKp(t) = Kp(t) - Kp(t-1)\n",
    "2. We can then use this transformed series of data in the autoregression model: ΔKp(t) = β0 + β1*ΔKp(t-1) + β2*ΔKp(t-2) + ... + β18*ΔKp(t-18) + ε(t), Note that the first differences model uses ΔKp instead of Kp and ΔKp(t-1) instead of Kp(t-1) in the autoregression model.\n",
    "3. We can estimate the coefficients of the autoregression model using the least squares method. To do this, we need to collect the last 19 values of the Kp index, i.e., Kp(t-18) to Kp(t), and use them to fit the model. The formula for the estimated coefficients is: β = (X'X)^(-1)X'Y,\n",
    "where X is a matrix of size (n-18) x 19) that contains the lagged Kp values, and Y is a vector of size (n-18) x 1 that includes the Kp index values for the same time period. The first row of X contains the Kp values at time t-18 to t-1, the second row includes the Kp values at time t-17 to t, and so on, up to the last row which contains the Kp values at time t-1 to t-18. The intercept term β0 is represented by a column of 1s in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ap_data(data_len,ap_3h):\n",
    "\n",
    "    ap_total=np.array([])\n",
    "    for ap3hstep in range(ap_start_index,data_len,hour_steps_for_height):\n",
    "        ap_elem=0\n",
    "\n",
    "        for step in range(past_steps,-1,-1):\n",
    "            ap_elem+=(ap_3h[ap3hstep-hour_steps_for_height*step]*X**step) #ap_x calculation with respect to last 33h + current time\n",
    "\n",
    "        ap_total=np.append(ap_total,np.repeat((1-X)*ap_elem,hour_steps_for_height)) # repeating for heights\n",
    "    return ap_total\n",
    "    \n",
    "def get_dst_data(dsts):\n",
    "    \n",
    "    dst_total=np.array([])\n",
    "    for dst_step in range(past_steps,len(dsts)): \n",
    "        \n",
    "        dst_elem=0\n",
    "        for step in range(past_steps,-1,-1):\n",
    "            dst_elem+=(dsts[dst_step-step]*X**step) #same calculation with ap\n",
    "\n",
    "        dst_total=np.append(dst_total,np.repeat((1-X)*dst_elem,hour_steps_for_height))\n",
    "\n",
    "    return dst_total\n",
    "        \n",
    "def get_kp_data(kps):\n",
    "    \n",
    "    delta_kp_total=np.array([])\n",
    "    delta_kps=np.array([])                     \n",
    "    for kp_index in range(hour_steps_for_height,len(kps),hour_steps_for_height):\n",
    "        delta_kps=np.append(delta_kps,kps[kp_index]-kps[kp_index-hour_steps_for_height])\n",
    "\n",
    "\n",
    "\n",
    "    delta_X = np.zeros((len(delta_kps)-18, delta_X_column_size))\n",
    "    delta_Y = np.array([])\n",
    "\n",
    "    for i in range(delta_step,len(delta_kps)):\n",
    "            delta_X[i-delta_step,:] = np.concatenate([[1],delta_kps[i-delta_step:i][::-1]])\n",
    "            delta_Y=np.append(delta_Y,delta_kps[i])\n",
    "\n",
    "    # compute the least squares solution\n",
    "    kp_betas = np.linalg.inv(delta_X.T.dot(delta_X)).dot(delta_X.T).dot(delta_Y)\n",
    "    for i in range(delta_step,len(delta_kps)):\n",
    "        delta_kp_total=np.append(delta_kp_total,np.repeat(get_new_delta_kps(kp_betas,delta_kps[i-delta_step:i][::-1]),hour_steps_for_height))\n",
    "    \n",
    "    return delta_kp_total\n",
    "\n",
    "\n",
    "def get_new_delta_kps(kp_betas,past_kps):\n",
    "    \n",
    "    kp_at_t=kp_betas[0]\n",
    "    for (beta,delta) in zip (kp_betas[1:],past_kps):\n",
    "        kp_at_t+=beta*delta\n",
    "    \n",
    "    return kp_at_t\n",
    "\n",
    "def get_r_delta_h(foF2):\n",
    "\n",
    "    r_delta_h_total=np.array([])\n",
    "    for foF2_index in range(hour_steps_for_height,len(foF2),hour_steps_for_height):\n",
    "        \n",
    "        delta1h=foF2[foF2_index]-foF2[foF2_index-1]\n",
    "        # delta1h_previous=foF2[foF2_index-1]-foF2[foF2_index-2]\n",
    "        # delta2h=delta1h-delta1h_previous          #they don't use delta2h in the paper,either.\n",
    "        \n",
    "        r_delta_h_total=np.append(r_delta_h_total,np.repeat(delta1h/foF2[foF2_index],hour_steps_for_height))\n",
    "\n",
    "    # print(r_delta_h.shape)\n",
    "    return r_delta_h_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in stations:\n",
    "\n",
    "    IRI_filename=st       # be sure of deleting all other non-numeric values before first section\n",
    "    year=IRI_filename[IRI_filename.index(\"_\")+1:IRI_filename.index(\".\")]\n",
    "    location=IRI_filename[:IRI_filename.index(\"_\")]\n",
    "    print(year,location)\n",
    "\n",
    "    date_start = pd.to_datetime(year+'-01-01')\n",
    "    date_end = pd.to_datetime(year+'-12-31')\n",
    "\n",
    "    column_names=[]  \n",
    "    \n",
    "    with open(datafolder+IRI_filename) as file:\n",
    "        for line in file.readlines()[1:59]:\n",
    "            column_names.append(line[line.find(' ')+1:].strip().replace(\" \", \"\"))\n",
    "\n",
    "        \n",
    "    df_IRI=pd.read_csv(datafolder+IRI_filename, skiprows=61,header=None,delim_whitespace=True)\n",
    "    df_IRI.columns=column_names\n",
    "\n",
    "    df_DIST=df_dst\n",
    "    df_DIST=df_DIST.loc[(df_DIST['DATE'] >= date_start) & (df_DIST['DATE'] <= date_end)]\n",
    "    df_DIST.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(\"IRI: \",df_IRI.shape)\n",
    "    print(\"DIST: \",df_DIST.shape, \"Matched Len: \", len(df_DIST)*hour_steps_for_height) #365x24 row\n",
    "\n",
    "    if(len(df_IRI)<len(df_DIST)*hour_steps_for_height):   \n",
    "        print(\"IRI has missing data for some hours. Syncronize!!\")\n",
    "\n",
    "\n",
    "    #deleting some values from df_dist in order to syncronize both dataframe\n",
    "    sync_i=0\n",
    "    while sync_i < len(df_IRI):\n",
    "        if int(df_IRI[\"Hour\"][sync_i]) != int(df_DIST[\"TIME\"][int(sync_i/hour_steps_for_height)][0:2]):\n",
    "            df_DIST.drop(sync_i/hour_steps_for_height, axis=0, inplace=True)\n",
    "            df_DIST=df_DIST.reset_index(drop=True)\n",
    "            continue\n",
    "        sync_i+=hour_steps_for_height\n",
    "\n",
    "    if(len(df_IRI)== len(df_DIST)*hour_steps_for_height):\n",
    "        print(\"syncronized\\n\")\n",
    "\n",
    "    # print(df_IRI.shape)\n",
    "    # print(df_DIST.shape) #365x24 row for each station\n",
    "    # for i in range(0,len(df_IRI),hour_steps_for_height):\n",
    "    #     print(df_IRI[\"Hour\"][i],df_DIST[\"TIME\"][i/hour_steps_for_height])\n",
    "\n",
    "\n",
    "    #the columns we use for generating new dataset.Season information is not included\n",
    "\n",
    "    dates=df_IRI.apply(lambda row: '-'.join([str(int(row['Year'])), str(int(row['Month'])),str(int(row['Day']))]), axis=1).to_numpy()\n",
    "    hours=df_IRI[\"Hour\"].to_numpy()\n",
    "    heights=df_IRI[\"Height,km\"].to_numpy()\n",
    "    solar_zenith_angles=df_IRI[\"Solar_zenith_angle,degree\"].to_numpy()\n",
    "    sun_spot_number=df_IRI[\"Rz12\"].to_numpy()\n",
    "    days_of_year= df_IRI[\"DOY\"].to_numpy()\n",
    "    declination=df_IRI[\"Declination,degree\"].to_numpy()\n",
    "    inclination=df_IRI[\"Dip,degree\"].to_numpy()\n",
    "    latitude=df_IRI[\"Latitude,degree\"].to_numpy()\n",
    "    longitude=df_IRI[\"Longitude,degree\"].to_numpy()\n",
    "    ap_3h=df_IRI[\"3-h_ap\"].to_numpy()\n",
    "    dsts=df_DIST[\"DST\"].to_numpy()\n",
    "    kps=df_IRI[\"3-h_kp\"].to_numpy()\n",
    "    foF2=df_IRI[\"foF2,MHz\"].to_numpy()\n",
    "\n",
    "    \n",
    "    hours_sin=np.sin(2*np.pi*hours/24) #in radian\n",
    "    hours_cos=np.cos(2*np.pi*hours/24) #in radian\n",
    "\n",
    "    solar_zenith_angles_sin=np.sin(2*np.pi*np.deg2rad(solar_zenith_angles)/360) #in radian (it  was originally degree)\n",
    "    solar_zenith_angles_cos=np.cos(2*np.pi*np.deg2rad(solar_zenith_angles)/360) #in radian\n",
    "\n",
    "    days_of_year_sin=np.sin(2*np.pi*days_of_year/360) #in radian\n",
    "    days_of_year_cos=np.cos(2*np.pi*days_of_year/360) #in radian\n",
    "\n",
    "    solar_activity_constant=np.full((len(df_IRI),),10.7) #it is constant, calculated as 10.7 cm.\n",
    "\n",
    "    #Geomagnetic Activity-->dst,kp,ap\n",
    "    ap_X=get_ap_data(len(hours),ap_3h)        # calculated w.r.t formula in the article.\n",
    "    dst_t=get_dst_data(dsts)                  # calculated w.r.t ap_X formula.\n",
    "    delta_kp_regressed=get_kp_data(kps)          # calculated w.r.t formula in the comment below\n",
    "\n",
    "    #Neutral Air Wind-->ds,dc,is\n",
    "\n",
    "    declination_sin=np.sin(2*np.pi*np.deg2rad(declination)/360) #in radian (it  was originally degree)\n",
    "    declination_cos=np.cos(2*np.pi*np.deg2rad(declination)/360) #in radian\n",
    "\n",
    "    inclination_sin=np.sin(2*np.pi*np.deg2rad(inclination)/360) #in radian (it  was originally degree)\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    "    #Geographic Coordinates-->lats,latc,lons,lonc\n",
    "\n",
    "    latitude_sin=np.sin(2*np.pi*np.deg2rad(latitude)/180) #in radian (it  was originally degree)\n",
    "    latitude_cos=np.cos(2*np.pi*np.deg2rad(latitude)/180) #in radian\n",
    "\n",
    "    longitude_sin=np.sin(2*np.pi*np.deg2rad(longitude)/360) #in radian (it  was originally degree)\n",
    "    longitude_cos=np.cos(2*np.pi*np.deg2rad(longitude)/360) #in radian\n",
    "\n",
    "###########################################################################################\n",
    "   \n",
    "    #r_delta_h\n",
    "    r_delta_h=get_r_delta_h(foF2)\n",
    "\n",
    "    #distance to the equator is added,too. in km\n",
    "    distance=np.abs(latitude)*69*1.6\n",
    "\n",
    "    #original foF2 is also being added as our output.\n",
    "    \n",
    "    ###########################################################################################\n",
    "    \n",
    "    #Generates foF2 dataset\n",
    "    # data_dictionary={\"Date\":dates[baseindex:],\"Hour\":hours[baseindex:],\"Height\":heights[baseindex:],\"HourS\":hours_sin[baseindex:],\"HourC\":hours_cos[baseindex:],\"CHIS\":solar_zenith_angles_sin[baseindex:],\\\n",
    "    #     \"CHIC\":solar_zenith_angles_cos[baseindex:],\"DNS\":days_of_year_sin[baseindex:],\"DNC\":days_of_year_cos[baseindex:],\"F10.7\":solar_activity_constant[baseindex:],\\\n",
    "    #         \"DST_t\":dst_t[baseindex2:],\"AP_x\":ap_X[baseindex2:],\"Delta_Kp\":delta_kp_regressed,\"DS\":declination_sin[baseindex:],\"DC\":declination_cos[baseindex:],\\\n",
    "    #             \"IS\":inclination_sin[baseindex:],\"LATS\":latitude_sin[baseindex:],\"LATC\":latitude_cos[baseindex:],\"LONS\":longitude_sin[baseindex:],\"LONC\":longitude_cos[baseindex:],\\\n",
    "    #                 \"R_Delta_H\":r_delta_h[baseindex3:],\"EqDistance\":distance[baseindex:],\"foF2\":foF2[baseindex:]}\n",
    "\n",
    "    #edited ver.\n",
    "\n",
    "    data_dictionary={\"Date\":dates[baseindex:],\"DOY\":days_of_year[baseindex:],\"Hour\":hours[baseindex:],\"Height\":heights[baseindex:],\"HourS\":hours_sin[baseindex:],\"HourC\":hours_cos[baseindex:],\"CHIS\":solar_zenith_angles_sin[baseindex:],\\\n",
    "    \"CHIC\":solar_zenith_angles_cos[baseindex:],\"DNS\":days_of_year_sin[baseindex:],\"DNC\":days_of_year_cos[baseindex:],\"Rz\":sun_spot_number[baseindex:],\"F10.7\":solar_activity_constant[baseindex:],\\\n",
    "        \"DST_t\":dst_t[baseindex2:],\"AP_x\":ap_X[baseindex2:],\"Delta_Kp\":delta_kp_regressed,\"DS\":declination_sin[baseindex:],\"DC\":declination_cos[baseindex:],\\\n",
    "            \"IS\":inclination_sin[baseindex:],\\\n",
    "                \"R_Delta_H\":r_delta_h[baseindex3:],\"EqDistance\":distance[baseindex:],\"foF2\":foF2[baseindex:]}\n",
    "\n",
    "    df_foF2=pd.DataFrame(data=data_dictionary)\n",
    "    print(df_foF2.shape)\n",
    "    print(df_foF2.head(10))\n",
    "\n",
    "    plot_data(location,year,df_foF2,3,5)\n",
    "\n",
    "    df_st_merged_foF2=pd.concat([df_st_merged_foF2,df_foF2])\n",
    "\n",
    "print(\"All stations merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves foF2 dataframe to the excel file\n",
    "\n",
    "save_prefix=datafolder+location+year\n",
    "\n",
    "if(len(stations)>1):\n",
    "    save_prefix=datafolder+year+\"_merged\"\n",
    "\n",
    "#df_st_merged_foF2.to_excel(save_prefix+\"_foF2.xlsx\",index=False)\n",
    "df_st_merged_foF2.to_csv(save_prefix+\"_foF2.txt\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eqpre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
