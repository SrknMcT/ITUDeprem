{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os \n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Flatten, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stations' data preparation\n",
    " \n",
    "data_folder=\"data\\\\\"\n",
    "stations=[\"POMIO2009_fOF2.txt\",\"MALUKU2009_fOF2.txt\",\"DARW2009_fOF2.txt\",\"SUL2009_fOF2.txt\",\"JOG22009_fOF2.txt\"\\\n",
    "          \"POMIO2015_fOF2.txt\",\"MALUKU2015_fOF2.txt\",\"DARW2015_fOF2.txt\",\"SUL2015_fOF2.txt\",\"JOG22015_fOF2.txt\"\\\n",
    "           \"2009_merged_foF2.txt\",\"2015_merged_foF2.txt\" ]\n",
    "\n",
    "\n",
    "stations_data={}\n",
    "\n",
    "for st in stations:\n",
    "    stations_data[st]=pd.read_csv(data_folder+st, sep =',')\n",
    "    print(\"\\n\\n--------------------------\"+st+\"--------------------------------------\\n\")\n",
    "    print(stations_data[st].head(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input-Output data preparation,split and scaling for ML Models\n",
    "hour_intervals=[1,2,12,24,36,48]   # end of this cell 0 is added for original input dataset\n",
    "hour_steps_for_height=39 #39 is the number of different heights\n",
    "\n",
    "X_train_base,X_val_base,X_test_base, y_train_base,y_val_base, y_test_base={},{},{},{},{},{}\n",
    "shift_size=hour_steps_for_height\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "#scaler =RobustScaler()\n",
    "\n",
    "\n",
    "for st in stations_data:\n",
    "    X,y={},{}\n",
    "    X[st] = stations_data[st].drop(['Date', 'Hour', 'Height', 'foF2'], axis=1).to_numpy()[:-shift_size]\n",
    "    y[st] = stations_data[st]['foF2'].to_numpy()[shift_size:]\n",
    "\n",
    "    hours_lcm=np.lcm.reduce(hour_intervals)\n",
    "    redundancy_index=int(((len(X[st])/hour_steps_for_height)%hours_lcm)*hour_steps_for_height)\n",
    "\n",
    "    X[st]=X[st][:-redundancy_index]\n",
    "    y[st]=y[st][:-redundancy_index]\n",
    "\n",
    "    split_index_lcm=np.floor((len(X[st])/(hour_steps_for_height*hours_lcm))*0.8)\n",
    "    split_index=int(split_index_lcm*hours_lcm*hour_steps_for_height)\n",
    "\n",
    "    #train_test split\n",
    "    X_train_base[st]=X[st][:split_index]\n",
    "    y_train_base[st]=y[st][:split_index]\n",
    "    X_test_base[st]=X[st][split_index:]\n",
    "    y_test_base[st]=y[st][split_index:]\n",
    "\n",
    "    #validation split--Final result = 0.8 train, 0.1 validation, 0.1 test,\n",
    "\n",
    "    val_split_lcm=np.floor((len(X_test_base[st])/(hour_steps_for_height*hours_lcm))*0.5)\n",
    "    val_split_index=int(val_split_lcm*hours_lcm*hour_steps_for_height)\n",
    "\n",
    "    X_val_base[st]=X_test_base[st][:val_split_index]\n",
    "    y_val_base[st]=y_test_base[st][:val_split_index]\n",
    "    X_test_base[st]=X_test_base[st][val_split_index:]\n",
    "    y_test_base[st]=y_test_base[st][val_split_index:]\n",
    "\n",
    "   \n",
    "    print(st)\n",
    "    print(\"Train: \",X_train_base[st].shape,y_train_base[st].shape,\"Validation:\",X_val_base[st].shape,y_val_base[st].shape,\"Test: \",X_test_base[st].shape,y_test_base[st].shape,\"\\n\")\n",
    "    \n",
    "    # #shuffling\n",
    "    # X_train_base[st], y_train_base[st] = shuffle( X_train_base[st],y_train_base[st])\n",
    "    # X_test_base[st], y_test_base[st] = shuffle(X_test_base[st], y_test_base[st])\n",
    "\n",
    "    #scaling\n",
    "    scaler.fit(X_train_base[st])\n",
    "    X_train_base[st] = scaler.transform(X_train_base[st])\n",
    "    X_val_base[st] = scaler.transform(X_val_base[st])\n",
    "    X_test_base[st] = scaler.transform(X_test_base[st])\n",
    "\n",
    "    print(\"Scaled X_train: \",X_train_base[st][0],\"\\n\") #prints first row of X_train\n",
    "\n",
    "hour_intervals.insert(0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or save results\n",
    "result_array = [['Train', 'Train', 'Train', 'Train','Validation', 'Validation', 'Validation', 'Validation', 'Test', 'Test', 'Test', 'Test'],\n",
    "          ['MSE', 'R2', 'MAE', 'MAPE','MSE', 'R2', 'MAE', 'MAPE','MSE', 'R2', 'MAE', 'MAPE']]\n",
    "results_df=pd.DataFrame(index=pd.MultiIndex.from_arrays(result_array,names=[\"ExperimentData\",\"Metric\"]))\n",
    "save_row_index=0\n",
    "save_column_index=0\n",
    "\n",
    "\n",
    "def save_error_to_df(model_name, data_type, mse_result,r2_result,mae_result,mape_result):\n",
    "    \n",
    "    results_df.loc[(data_type, 'MSE'), model_name] = mse_result\n",
    "    results_df.loc[(data_type, 'R2'), model_name] = r2_result\n",
    "    results_df.loc[(data_type, 'MAE'), model_name] = mae_result\n",
    "    results_df.loc[(data_type, 'MAPE'), model_name] = mape_result\n",
    "\n",
    "def printError(model_name, data_type, y_in, pred_in):\n",
    "    # Calculate ensemble metrics\n",
    "    print(\"\\n\",model_name + \" \" + data_type +' Model Performance:'+\"\\n\")\n",
    "    _mse = mean_squared_error(y_in, pred_in)\n",
    "    _r2 = r2_score(y_in, pred_in)\n",
    "    _mae = mean_absolute_error(y_in, pred_in)\n",
    "    _mape = np.mean(np.abs((y_in - pred_in) / y_in)) * 100\n",
    "    \n",
    "    print('MSE:', _mse)\n",
    "    print('R^2:', _r2)\n",
    "    print('MAE:', _mae)\n",
    "    print('MAPE:', _mape)\n",
    "    #print('MSE:', _mse ,'\\tR^2:', _r2, '\\tMAE:', _mae, '\\tMAPE:', _mape)\n",
    "\n",
    "    save_error_to_df(model_name,data_type,round(_mse,2),round(_r2,2),round(_mae,2),round(_mape,2))\n",
    "\n",
    "def save_error_to_excel(st_result_header,first_run):\n",
    "    global save_row_index\n",
    "    global save_column_index\n",
    "    st_header_df=pd.DataFrame(data=st_result_header)\n",
    "\n",
    "    if first_run:\n",
    "        #Export all results to an excel file\n",
    "        st_header_df.to_excel(data_folder+\"foF2_ML_Experiments.xlsx\",startrow=save_row_index,startcol=save_column_index,index=False,header=False)\n",
    "        save_row_index+=1\n",
    "      \n",
    "        with pd.ExcelWriter(data_folder+\"foF2_ML_Experiments.xlsx\", mode=\"a\",if_sheet_exists=\"overlay\") as writer:\n",
    "            results_df.to_excel(writer,startrow=save_row_index,startcol=save_column_index)\n",
    "        save_row_index+=16\n",
    "    else :\n",
    "        with pd.ExcelWriter(data_folder+\"foF2_ML_Experiments.xlsx\", mode=\"a\",if_sheet_exists=\"overlay\") as writer:\n",
    "            st_header_df.to_excel(writer,startrow=save_row_index,startcol=save_column_index,index=False,header=False)\n",
    "            save_row_index+=1\n",
    "            results_df.to_excel(writer,startrow=save_row_index,startcol=save_column_index)\n",
    "            save_row_index+=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run=True\n",
    "\n",
    "for hour_interval in hour_intervals:\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test=X_train_base.copy(),X_val_base.copy(),X_test_base.copy(),y_train_base.copy(),y_val_base.copy(),y_test_base.copy()\n",
    "  \n",
    "\n",
    "    for st in stations_data:\n",
    "\n",
    "        st_result_header=[st[:st.rfind('_')]] #Results' header\n",
    "\n",
    "        if hour_interval > 0 :\n",
    "            \n",
    "            st_result_header=[st[:st.rfind('_')]+\"_\"+str(hour_interval)+\"h\"]\n",
    "\n",
    "            # Create a boolean mask for rows to be dropped\n",
    "            mask_train = np.zeros(X_train_base[st].shape[0], dtype=bool)\n",
    "            mask_val = np.zeros(X_val_base[st].shape[0], dtype=bool)\n",
    "            mask_test = np.zeros(X_test_base[st].shape[0], dtype=bool)\n",
    "            interval_index=0\n",
    "\n",
    "            while interval_index < len(X_train_base[st]):\n",
    "                mask_train[interval_index]=True;\n",
    "                interval_index+=hour_steps_for_height*hour_interval\n",
    "\n",
    "            X_train[st]=np.copy(X_train_base[st][mask_train])\n",
    "            y_train[st]=np.copy(y_train_base[st][mask_train])\n",
    "\n",
    "            interval_index=0\n",
    "\n",
    "            while interval_index < len(X_val_base[st]):\n",
    "                mask_val[interval_index]=True;\n",
    "                interval_index+=hour_steps_for_height*hour_interval\n",
    "\n",
    "            X_val[st]=np.copy(X_val_base[st][mask_val])\n",
    "            y_val[st]=np.copy(y_val_base[st][mask_val])\n",
    "\n",
    "            \n",
    "            interval_index=0\n",
    "\n",
    "            while interval_index < len(X_test_base[st]):\n",
    "                mask_test[interval_index]=True;\n",
    "                interval_index+=hour_steps_for_height*hour_interval\n",
    "\n",
    "            X_test[st]=np.copy(X_test_base[st][mask_test])\n",
    "            y_test[st]=np.copy(y_test_base[st][mask_test])\n",
    "\n",
    "        \n",
    "        \n",
    "        #Linear Regression Model\n",
    "\n",
    "        reg = LinearRegression().fit(X_train[st], y_train[st])\n",
    "\n",
    "        reg_predTrain = reg.predict(X_train[st]) \n",
    "        reg_errorTrain = y_train[st] - reg_predTrain\n",
    "        printError(\"Linear Regression\",\"Train\", y_train[st], reg_predTrain)\n",
    "\n",
    "        reg_predVal = reg.predict(X_val[st]) \n",
    "        reg_errorVal = y_val[st] - reg_predVal\n",
    "        printError(\"Linear Regression\",\"Validation\", y_val[st], reg_predVal)\n",
    "\n",
    "        reg_predTest = reg.predict(X_test[st])\n",
    "        reg_errorTest = y_test[st] - reg_predTest\n",
    "        printError(\"Linear Regression\",\"Test\", y_test[st], reg_predTest)\n",
    "\n",
    "\n",
    "        # Gradient Boosting model\n",
    "\n",
    "        gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "        gb.fit(X_train[st], y_train[st])\n",
    "\n",
    "        gb_predTrain = gb.predict(X_train[st]) \n",
    "        gb_errorTrain = y_train[st] - gb_predTrain\n",
    "        printError(\"Gradient Boosting\",\"Train\", y_train[st], gb_predTrain)\n",
    "\n",
    "        gb_predVal = gb.predict(X_val[st]) \n",
    "        gb_errorVal = y_val[st] - gb_predVal\n",
    "        printError(\"Gradient Boosting\",\"Validation\", y_val[st], gb_predVal)\n",
    "\n",
    "        gb_predTest = gb.predict(X_test[st])\n",
    "        gb_errorTest = y_test[st] - gb_predTest\n",
    "        printError(\"Gradient Boosting\",\"Test\", y_test[st], gb_predTest)\n",
    "\n",
    "\n",
    "        # MLP model\n",
    "\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "        mlp.fit(X_train[st], y_train[st])\n",
    "\n",
    "        mlp_predTrain = mlp.predict(X_train[st])\n",
    "        mlp_errorTrain = y_train[st] - mlp_predTrain\n",
    "        printError(\"MLP\",\"Train\", y_train[st], mlp_predTrain)\n",
    "\n",
    "        mlp_predVal = mlp.predict(X_val[st])\n",
    "        mlp_errorVal = y_val[st] - mlp_predVal\n",
    "        printError(\"MLP\",\"Validation\", y_val[st], mlp_predVal)\n",
    "\n",
    "        mlp_predTest = mlp.predict(X_test[st])\n",
    "        mlp_errorTest = y_test[st] - mlp_predTest\n",
    "        printError(\"MLP\",\"Test\",y_test[st], mlp_predTest)\n",
    "\n",
    "\n",
    "        # Random Forest model\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "        rf.fit(X_train[st], y_train[st])\n",
    "        \n",
    "        rf_predTrain = rf.predict(X_train[st])\n",
    "        rf_errorTrain = y_train[st] - rf_predTrain\n",
    "        printError(\"Random Forest\",\"Train\", y_train[st], rf_predTrain)\n",
    "\n",
    "        rf_predVal = rf.predict(X_val[st])\n",
    "        rf_errorVal = y_val[st] - rf_predVal\n",
    "        printError(\"Random Forest\",\"Validation\", y_val[st], rf_predVal)\n",
    "\n",
    "        rf_predTest = rf.predict(X_test[st])\n",
    "        rf_errorTest = y_test[st] - rf_predTest\n",
    "        printError(\"Random Forest\",\"Test\", y_test[st], rf_predTest)\n",
    "\n",
    "\n",
    "        # CatBoost model\n",
    "\n",
    "        catboost = CatBoostRegressor(iterations=1000, learning_rate=0.1, random_seed=42)\n",
    "        catboost.fit(X_train[st], y_train[st], silent=True)\n",
    "        \n",
    "        catboost_predTrain = catboost.predict(X_train[st]) \n",
    "        catboost_errorTrain = y_train[st] - catboost_predTrain\n",
    "        printError(\"CatBoost\",\"Train\", y_train[st], catboost_predTrain)\n",
    "\n",
    "        catboost_predVal = catboost.predict(X_val[st]) \n",
    "        catboost_errorVal = y_val[st] - catboost_predVal\n",
    "        printError(\"CatBoost\",\"Validation\", y_val[st], catboost_predVal)\n",
    "\n",
    "        catboost_predTest = catboost.predict(X_test[st])\n",
    "        catboost_errorTest = y_test[st] - catboost_predTest\n",
    "        printError(\"CatBoost\",\"Test\", y_test[st], catboost_predTest)\n",
    "\n",
    "        #LSTM Model\n",
    "\n",
    "        X_lstm_train = np.expand_dims(X_train[st], axis = 2)\n",
    "        X_lstm_val = np.expand_dims(X_val[st], axis = 2)\n",
    "        X_lstm_test = np.expand_dims(X_test[st], axis = 2)\n",
    "        y_lstm_train = np.expand_dims(y_train[st], axis = 1)\n",
    "        y_lstm_val = np.expand_dims(y_val[st], axis = 1)\n",
    "        y_lstm_test = np.expand_dims(y_test[st], axis = 1)\n",
    "\n",
    "        model_lstm = Sequential()\n",
    "        model_lstm.add(LSTM(36, activation='relu', input_shape=(X_lstm_train.shape[1], X_lstm_train.shape[2])))\n",
    "        model_lstm.add(Dense(12))\n",
    "        model_lstm.add(Dense(1))\n",
    "        model_lstm.compile(optimizer=Adam(0.0003), loss='mae', metrics=['mae'])\n",
    "\n",
    "        lstm_history = model_lstm.fit(X_lstm_train, y_lstm_train, epochs=50,batch_size=78, verbose=2, use_multiprocessing=True)\n",
    "        \n",
    "        lstm_predTrain = model_lstm.predict(X_lstm_train) \n",
    "        lstm_errorTrain = y_lstm_train - lstm_predTrain\n",
    "        printError(\"LSTM\",\"Train\", y_lstm_train, lstm_predTrain)\n",
    "\n",
    "        lstm_predVal = model_lstm.predict(X_lstm_val) \n",
    "        lstm_errorVal = y_lstm_val - lstm_predVal\n",
    "        printError(\"LSTM\",\"Validation\", y_lstm_val, lstm_predVal)\n",
    "\n",
    "        lstm_predTest = model_lstm.predict(X_lstm_test)\n",
    "        lstm_errorTest = y_lstm_test - lstm_predTest\n",
    "        printError(\"LSTM\",\"Test\", y_lstm_test, lstm_predTest)\n",
    "\n",
    "\n",
    "        # Ensemble model with CNN\n",
    "\n",
    "        X_ensembleTrain = np.column_stack((reg_predTrain[1:],rf_predTrain[1:], gb_predTrain[1:], mlp_predTrain[1:], catboost_predTrain[1:],lstm_predTrain[1:]))\n",
    "        X_ensembleValidation = np.column_stack((reg_predVal[1:],rf_predVal[1:], gb_predVal[1:], mlp_predVal[1:], catboost_predVal[1:],lstm_predVal[1:]))\n",
    "        X_ensembleTest = np.column_stack((reg_predTest[1:],rf_predTest[1:], gb_predTest[1:], mlp_predTest[1:], catboost_predTest[1:],lstm_pred_test[1:]))\n",
    "        # Reshape the input data from 2D to 3D\n",
    "        X_ensembleTrain = np.reshape(X_ensembleTrain, (X_ensembleTrain.shape[0], X_ensembleTrain.shape[1], 1))\n",
    "        X_ensembleValidation = np.reshape(X_ensembleValidation, (X_ensembleValidation.shape[0], X_ensembleValidation.shape[1], 1))\n",
    "        X_ensembleTest = np.reshape(X_ensembleTest, (X_ensembleTest.shape[0], X_ensembleTest.shape[1], 1))\n",
    "        \n",
    "        # Define the CNN model\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_ensembleTrain.shape[1], 1)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "        # Train the model\n",
    "        history = model.fit(X_ensembleTrain, y_train[st][1:], epochs=50, batch_size=32)\n",
    "\n",
    "        # Evaluate the model on the training and test sets\n",
    "        cnn_predTrain = model.predict(X_ensembleTrain)\n",
    "        cnn_predVal = model.predict(X_ensembleValidation)\n",
    "        cnn_predTest = model.predict(X_ensembleTest)\n",
    "\n",
    "        printError(\"CNN\",\"Train\", y_train[st][1:].flatten(), cnn_predTrain.flatten())\n",
    "        printError(\"CNN\",\"Validation\", y_val[st][1:].flatten(), cnn_predVal.flatten())\n",
    "        printError(\"CNN\",\"Test\",y_test[st][1:].flatten(), cnn_predTest.flatten())\n",
    "\n",
    "        \n",
    "        #Ensemble model with CNN+Errors\n",
    "\n",
    "        # Combine errors from previous models\n",
    "        error_dfTrain = pd.DataFrame({'reg_error': reg_errorTrain[:-1],'rf_error': rf_errorTrain[:-1], 'gb_error': gb_errorTrain[:-1], 'mlp_error': mlp_errorTrain[:-1],\\\n",
    "                                    'catboost_error': catboost_errorTrain[:-1],'lstm_error':lstm_errorTrain[:-1]})\n",
    "        \n",
    "        error_dfVal = pd.DataFrame({'reg_error': reg_errorVal[:-1],'rf_error': rf_errorVal[:-1], 'gb_error': gb_errorVal[:-1], 'mlp_error': mlp_errorVal[:-1],\\\n",
    "                            'catboost_error': catboost_errorVal[:-1],'lstm_error':lstm_errorVal[:-1]})\n",
    "\n",
    "        error_dfTest = pd.DataFrame({'reg_error': reg_errorTest[:-1],'rf_error': rf_errorTest[:-1], 'gb_error': gb_errorTest[:-1], 'mlp_error': mlp_errorTest[:-1],\\\n",
    "                                'catboost_error': catboost_errorTest[:-1],'lstm_error':lstm_errorTest[:-1]})\n",
    "\n",
    "        # Ensemble model\n",
    "        X_ensembleTrain = np.column_stack((reg_predTrain[1:],rf_predTrain[1:], gb_predTrain[1:], mlp_predTrain[1:], catboost_predTrain[1:],lstm_predTrain[1:], error_dfTrain))\n",
    "        X_ensembleVal = np.column_stack((reg_predVal[1:],rf_predVal[1:], gb_predVal[1:], mlp_predval[1:], catboost_predVal[1:],lstm_predVal[1:], error_dfVal))\n",
    "        X_ensembleTest = np.column_stack((reg_predTest[1:],rf_predTest[1:], gb_predTest[1:], mlp_predTest[1:], catboost_predTest[1:],lstm_predTest[1:],error_dfTest))\n",
    "        # Reshape the input data from 2D to 3D\n",
    "        X_ensembleTrain = np.reshape(X_ensembleTrain, (X_ensembleTrain.shape[0], X_ensembleTrain.shape[1], 1))\n",
    "        X_ensembleVal = np.reshape(X_ensembleVal, (X_ensembleVal.shape[0], X_ensembleVal.shape[1], 1))\n",
    "        X_ensembleTest = np.reshape(X_ensembleTest, (X_ensembleTest.shape[0], X_ensembleTest.shape[1], 1))\n",
    "\n",
    "        # Train the already  generated cnn model\n",
    "        history = model.fit(X_ensembleTrain, y_train[st][1:], epochs=50, batch_size=32)\n",
    "\n",
    "        # Evaluate the model on the training and test sets\n",
    "        cnn_predTrain2 = model.predict(X_ensembleTrain)\n",
    "        cnn_predVal2 = model.predict(X_ensembleVal)\n",
    "        cnn_predTest2 = model.predict(X_ensembleTest)\n",
    "\n",
    "        printError(\"CNNwithErrors\",\"Train\",  y_train[st][1:].flatten(), cnn_predTrain2.flatten())\n",
    "        printError(\"CNNwithErrors\",\"Validation\",  y_val[st][1:].flatten(), cnn_predVal2.flatten())\n",
    "        printError(\"CNNwithErrors\",\"Test\",  y_test[st][1:].flatten(), cnn_predTest2.flatten())\n",
    "        \n",
    "\n",
    "        save_error_to_excel(st_result_header,first_run)\n",
    "        first_run=False\n",
    "    \n",
    "\n",
    "    save_column_index+=10\n",
    "    save_row_index=0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50ca854c4c3efc08975ea3cb237e66ea0431a08d0f81600bb6a2a9be70ab2e48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
